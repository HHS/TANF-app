groups:
  - name: database.alerts
    rules:
      - alert: DevDatabaseDown
        expr: last_over_time(pg_up{job="postgres-dev"}[1m]) == 0 or last_over_time(up{job="postgres-dev"}[1m]) == 0
        labels:
          severity: CRITICAL
        annotations:
          summary: "The {{ $labels.service }} service is down."
          description: "The {{ $labels.service }} service in the {{ $labels.env }} environment has been down for more than 1 minute."
      - alert: StagingDatabaseDown
        expr: last_over_time(pg_up{job="postgres-staging"}[1m]) == 0 or last_over_time(up{job="postgres-staging"}[1m]) == 0
        labels:
          severity: ERROR
        annotations:
          summary: "The {{ $labels.service }} service is down."
          description: "The {{ $labels.service }} service in the {{ $labels.env }} environment has been down for more than 1 minute."
      - alert: ProductionDatabaseDown
        expr: last_over_time(pg_up{job="postgres-production"}[1m]) == 0 or last_over_time(up{job="postgres-production"}[1m]) == 0
        labels:
          severity: CRITICAL
        annotations:
          summary: "The {{ $labels.service }} service is down."
          description: "The {{ $labels.service }} service in the {{ $labels.env }} environment has been down for more than 1 minute."
  - name: backend.alerts
    rules:
    - alert: DevEnvironmentBackendDown
      expr: last_over_time(up{job=~"tdp-backend.*", job!~".*prod", job!~".*staging"}[5m]) == 0
      labels:
        severity: ERROR
      annotations:
          summary: "The {{ $labels.service }} service is down."
          description: "The {{ $labels.service }} service in the {{ $labels.env }} environment has been down for more than 5 minutes."
    - alert: StagingBackendDown
      expr: last_over_time(up{job=~"tdp-backend-staging"}[1m]) == 0
      labels:
        severity: ERROR
      annotations:
          summary: "The {{ $labels.service }} service is down."
          description: "The {{ $labels.service }} service in the {{ $labels.env }} environment has been down for more than 1 minute."
    - alert: ProductionBackendDown
      expr: last_over_time(up{job=~"tdp-backend-prod"}[1m]) == 0
      labels:
        severity: CRITICAL
      annotations:
          summary: "The {{ $labels.service }} service is down."
          description: "The {{ $labels.service }} service in the {{ $labels.env }} environment has been down for more than 1 minute."
  - name: plg.alerts
    rules:
    - alert: LokiDown
      expr: last_over_time(up{job="loki"}[1m]) == 0
      labels:
        severity: ERROR
      annotations:
          summary: "The {{ $labels.service }} service is down."
          description: "The {{ $labels.service }} service in the {{ $labels.env }} environment has been down for more than 1 minute."
    - alert: GrafanaDown
      expr: last_over_time(up{job="grafana"}[1m]) == 0
      labels:
        severity: ERROR
      annotations:
          summary: "The {{ $labels.service }} service is down."
          description: "The {{ $labels.service }} service in the {{ $labels.env }} environment has been down for more than 1 minute."
  - name: app.alerts
    rules:
    - alert: UpTime
      expr: avg_over_time(up[1d]) < 0.95
      for: 30m
      labels:
        severity: WARNING
      annotations:
          summary: "The {{ $labels.service }} service has a uptime warning."
          description: "The {{ $labels.service }} service in the {{ $labels.env }} environment is not maintaining 95% uptime."
  - name: celery.alerts
    rules:
      - alert: CeleryTaskHighFailRate
        annotations:
          description: "More than 5% tasks failed for the task {{ $labels.job }}/{{ $labels.queue_name }}/{{ $labels.name }} the past 10m."
          summary: "Celery high task fail rate."
        expr: |
          sum(
            increase(
              celery_task_failed_total{
                job=~"celery|celery-exporter",
                queue_name!~"None",
                name!~"None"
              }[10m]
            )
          )  by (job, queue_name, name)
          /
          (
            sum(
              increase(
                celery_task_failed_total{
                  job=~"celery|celery-exporter",
                  queue_name!~"None",
                  name!~"None"
                }[10m]
              )
            )  by (job, queue_name, name)
            +
            sum(
              increase(
                celery_task_succeeded_total{
                  job=~"celery|celery-exporter",
                  queue_name!~"None",
                  name!~"None"
                }[10m]
              )
            )  by (job, queue_name, name)
          )
          * 100 > 5
        for: 1m
        labels:
          severity: WARNING
      - alert: CeleryHighQueueLength
        annotations:
          description: "More than 15 tasks in the queue {{ $labels.job }}/{{ $labels.queue_name }} the past 20m."
          summary: "Celery high queue length."
        expr: |
          sum(
            celery_queue_length{
              job=~"celery|celery-exporter",
              queue_name!~"None"
            }
          )  by (job, queue_name)
          > 15
        for: 20m
        labels:
          severity: WARNING
      - alert: CeleryWorkerDown
        annotations:
          description: "The Celery worker {{ $labels.job }}/{{ $labels.hostname }} is offline."
          summary: "A Celery worker is offline."
        expr: celery_worker_up{job=~"celery|celery-exporter"} == 0
        for: 5m
        labels:
          severity: WARNING
